Verbal summary of the implementation
  Code retrieves data from the YouTube Data API, stores it in a MongoDB database, and later transfers some of that data to a PostgreSQL database for analysis

1. Importing necessary libraries, including modules for working with Google's YouTube Data API, MongoDB, PostgreSQL, and Streamlit for building web interfaces.

2.Setting up the YouTube Data API by providing an API key and specifying the API service and version.

3.Defining functions for various tasks:
  dropdownlist(): 
    Creates a list of channel names for selection.
  duration(data): 
    Converts a video duration in ISO 8601 format to hours.
  get_channel_details(channel_id): 
    Retrieves details about a YouTube channel using the API.
  get_playlist_details(playlist_id): 
    Fetches video IDs from a channel's playlist id.
  get_video_details(video_ids): 
    Retrieves details of videos using their IDs.
  get_comment_details(video_ids): 
    Gathers comments on videos
  data_to_mongo(data): 
    Stores channel, video, and comment details in a MongoDB database.
  retrive(data): 
    Retrieves data from MongoDB and inserts it into a PostgreSQL database.
  analysis(data): 
    Executes SQL queries and displays results in Streamlit.

4.Setting up a Streamlit web application with sections for data collection, data conversion, and data analysis.

5.In the data collection section, users can input a YouTube channel ID and click a button to retrieve data from the API, which is then stored in MongoDB.

6.In the data conversion section, users can select a channel name from a dropdown list and click a button to transfer data from MongoDB to PostgreSQL.

7.In the data analysis section, users can choose from various predefined questions related to the stored data, and the script executes SQL queries to retrieve and display relevant information using Streamlit's interface.

Overall, this script provides a user-friendly way to collect, store, and analyze YouTube channel data using Python and various APIs and databases. It combines multiple technologies to create a data pipeline from web scraping to data visualization.
